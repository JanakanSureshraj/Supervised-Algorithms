Question 2 Sunburn 

Parent Entropy *Look at the result column
Enrtopy(S)= -3/8log2(3/8)-5/8log2(5/8)
          = 0.5306 + 0.4238
          = 0.9544

Entropy for each attribute 

Hair Attribute 
Entropy(blonde)= -2/4log2(2/4)-2/4log2(2/4)
               = 1
Entropy(brown) = -0-3/3log2(3/3)
               = 0
Entropy(red)   = -1/1log2(1/1)-0
               = 0
Entropy(hair)  = individual entropy * weights
               = 4/8*1 + 3/8*0 + 1/8*0
               = 0.5 
Information Gain(hair)= 0.9544-0.5
                      = 0.4544

Height Attibute 
Entropy(average)= -2/3log2(2/3)-1/3log2(1/3)
                = 0.389975 + 0.5283
                = 0.9182
Entropy(tall)    = 0- 2/2log2(2/2)
                = 0
Entropy(short)  = -1/3log2(1/3)-2/3log2(2/3)
                = 0.389975 + 0.5283
                = 0.9182
Entropy(height)   = 3/8*0.9182 + 2/8*0 + 3/8*0.9182
                = 0.68865
Information Gain(height)= 0.9544-0.68865
                        = 0.26575

Weight Attribute 
Entropy(light)   = -1/2log2(1/2)-1/2log2(1/2)
                 = 1
Entropy(average) = -1/3log2(1/3)-2/3log2(2/3)
                 = 0.389975 + 0.5283
                 = 0.9182
Entropy(heavy)   = -2/3log2(2/3)-1/3log2(1/3)
                 = 0.389975 + 0.5283
                 = 0.9182
Entropy(weight)  = 2/8*1 + 3/8*0.9812 + 3/8*0.9812
                 = 0.9387	
Information Gain(weight)= 0.9544-0.9387
                        = 0.0157

Lotion Attribute 
Entropy(yes)   = -0-3/3log2(3/3)
                 = 0
Entropy(no) = -3/5log2(3/5)-2/5log2(2/5)
                 = 0.4422 + 0.5283
                 = 0.97097
Entropy(lotion)= 3/8*0 + 5/8*0.97097
               = 0.607
Information Gain(lotion)= 0.9544-0.607
                        = 0.3475
**HAIR IS THE ROOT NODE SINCE IT HAS THE HIGHEST INFORMATION GAIN. 

Constructing the tree further... 
When the decision tree is constructed, both the red and brown values lead to a single class. So, they can be mapped directly. However, for the blonde value, another attribute needs to be identified as a selector as well. 

Only the "blonde" samples in the dataset need to be used. 
Name	Hair	      Height   Weight	Lotion       Sunburned
Sarah	Blonde	Average  Light	No	       Yes
Dana	Blonde	Tall	   Average	Yes	       No
Annie	Blonde	Short	   Average	No	       Yes
Katie	Blonde	Short	   Light	Yes	       No

Entropy(blonde)= -2/4log2(2/4)-2/4log2(2/4) 
               = 1

Entropy of each attribute needs to be calculated. 
Height Attribute
Entropy(average)= -1/1log2(1/1)-0
                = 0
Entropy(Tall)   = -1/1log2(1/1)-0		
                = 0
Entropy(Short)  = -1/2log2(1/2)-1/2log2(1/2)
                = 1
Entropy(height) = 1/4*0 + 1/4*0 + 2/4*1
                = 0.5
Information Gain(height)= 1-0.5 
                        = 0.5

Weight Attribute

Entropy(light)  = -1/2log2(1/2)-1/2log2(1/2)
                = 1
Entropy(average)= -1/2log2(1/2)-1/2log2(1/2)
                = 1
Entropy(weight) = 2/4*1 + 2/4*1 
                = 1
Information Gain(weight)= 1-1
                        = 0
Lotion Attribute

Entropy(yes)    = 0-2/2log2(2/2)
                =0
Entropy(no)     = -2/2log2(2/2)-0
                =0 
Entropy(lotion) = 2/4*0 + 2/4*0
                = 0
Information Gain(lotion)= 1-0
                        = 1

*LOTION IS CHOSEN SINCE IT HAS THE HIGHEST INFORMATION GAIN. 

Construct the decision tree further and show the classes accordingly. 